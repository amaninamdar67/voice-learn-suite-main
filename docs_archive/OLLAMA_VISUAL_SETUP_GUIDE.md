# Ollama Setup - Visual Guide

## Step 1: Download Ollama

```
Visit: https://ollama.ai
         â†“
    Click "Download"
         â†“
    Choose your OS (Windows/Mac/Linux)
         â†“
    Run the installer
         â†“
    Restart your computer
```

---

## Step 2: Verify Installation

### Windows:
```
Open Command Prompt
         â†“
Type: ollama --version
         â†“
You should see: ollama version 0.1.0 (or similar)
```

### Mac/Linux:
```
Open Terminal
         â†“
Type: ollama --version
         â†“
You should see: ollama version 0.1.0 (or similar)
```

---

## Step 3: Start Ollama

### Windows (Easiest):
```
Project Root Folder
         â†“
Find: START_DEEPSEEK_OLLAMA.bat
         â†“
Double-click it
         â†“
Command window opens
         â†“
Wait for: "Ollama service running on port 11434"
         â†“
Keep window open âœ“
```

### Mac/Linux:
```
Open Terminal
         â†“
Type: ollama pull deepseek-r1:1.5b
         â†“
Wait for download (2-5 minutes first time)
         â†“
Type: ollama serve
         â†“
Wait for: "Ollama service running"
         â†“
Keep terminal open âœ“
```

---

## Step 4: Start Your Application

```
Open another Command Prompt/Terminal
         â†“
Navigate to project folder
         â†“
Type: npm run dev
         â†“
Application starts on http://localhost:5173
```

---

## Step 5: Use AI Tutor

```
Application is running
         â†“
Look for ğŸ¤– button (bottom-right corner)
         â†“
Click it
         â†“
AI Tutor opens
         â†“
Type your question
         â†“
Click Send or press Enter
         â†“
Get AI response! âœ“
```

---

## What's Happening Behind the Scenes

```
You type a question
         â†“
Frontend sends to: http://localhost:3001/api/ai-tutor/chat
         â†“
Backend receives message
         â†“
Backend sends to: http://localhost:11434/api/generate
         â†“
Ollama (running locally) processes with DeepSeek model
         â†“
Ollama returns response
         â†“
Backend sends response back to frontend
         â†“
You see the answer! âœ“
```

---

## File Structure

```
Your Project
â”œâ”€â”€ START_DEEPSEEK_OLLAMA.bat          â† Windows: Double-click this
â”œâ”€â”€ OLLAMA_SETUP_INSTRUCTIONS.md       â† Detailed guide
â”œâ”€â”€ AI_TUTOR_QUICK_START.md            â† Quick reference
â”œâ”€â”€ AI_TUTOR_SETUP_SUMMARY.txt         â† This summary
â”œâ”€â”€ backend/
â”‚   â””â”€â”€ server.js                      â† API endpoints
â””â”€â”€ src/
    â””â”€â”€ components/
        â””â”€â”€ AITutor/
            â””â”€â”€ AITutorEnhanced.tsx    â† Frontend component
```

---

## Ports Used

```
Your Application:     http://localhost:5173
Backend Server:       http://localhost:3001
Ollama Service:       http://localhost:11434
```

Make sure these ports are not blocked by your firewall!

---

## First Time Setup Timeline

```
1. Download Ollama:           5 minutes
2. Install Ollama:            2 minutes
3. Restart computer:          1 minute
4. Run START_DEEPSEEK_OLLAMA: 2-5 minutes (downloads model)
5. Start application:         1 minute
6. First AI response:         5-10 seconds (model loads)
7. Subsequent responses:      2-5 seconds

Total: ~20-30 minutes first time
```

---

## Common Issues & Solutions

### Issue: "Cannot connect to Ollama"
```
Problem: Ollama not running
Solution: 
  Windows â†’ Double-click START_DEEPSEEK_OLLAMA.bat
  Mac/Linux â†’ Run: ollama serve
```

### Issue: "Model not found"
```
Problem: Model downloading
Solution: Wait 2-5 minutes for download to complete
```

### Issue: "Out of memory"
```
Problem: Not enough RAM
Solution: 
  1. Close other applications
  2. Use smaller model: deepseek-r1:1b
  3. Update backend/server.js line ~1040
```

### Issue: Very slow responses
```
Problem: Normal for local AI
Solution: 
  1. First response is slower (model loads)
  2. Subsequent responses are faster
  3. Close other apps to free RAM
```

---

## Checking Status

### Is Ollama running?

**Windows:**
```
Look for the command window with "Ollama service running"
OR
Open Command Prompt and type: ollama list
```

**Mac/Linux:**
```
Look for the terminal with "Ollama service running"
OR
Open Terminal and type: ollama list
```

### Is the backend running?

```
Check terminal where you ran: npm run dev
Should show: "Backend server running on http://localhost:3001"
```

### Is the application running?

```
Open browser and go to: http://localhost:5173
Should see your application
```

---

## Performance Tips

1. **Keep Ollama running** - Don't close the service window
2. **Close other apps** - Free up RAM for better performance
3. **First response is slow** - Model loads into memory
4. **Subsequent responses are faster** - Model stays loaded
5. **Use GPU** - If available, Ollama uses it automatically

---

## Next Steps

1. âœ“ Install Ollama
2. âœ“ Run START_DEEPSEEK_OLLAMA.bat (Windows) or ollama serve (Mac/Linux)
3. âœ“ Run npm run dev
4. âœ“ Click ğŸ¤– button
5. âœ“ Start chatting!

---

## Need Help?

- **Ollama Issues**: https://github.com/ollama/ollama
- **DeepSeek Models**: https://huggingface.co/deepseek-ai
- **Model Library**: https://ollama.ai/library

Enjoy your AI Tutor! ğŸš€

